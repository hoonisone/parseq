{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from strhub.data.module import SceneTextDataModule\n",
    "from strhub.models.parseq.system import *\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from strhub.models.utils import load_from_checkpoint, parse_model_args\n",
    "# Load model and image transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do(model, sample_path):\n",
    "    img_transform = SceneTextDataModule.get_transform(model.hparams.img_size)\n",
    "    img = Image.open(sample_path).convert('RGB')\n",
    "    # img.show()\n",
    "    img = img_transform(img).unsqueeze(0)\n",
    "\n",
    "    logits = model(img)\n",
    "    logits.shape  # torch.Size([1, 26, 95]), 94 characters + [EOS] symbol\n",
    "\n",
    "    # Greedy decoding\n",
    "    pred = logits.softmax(-1)\n",
    "\n",
    "    label, confidence = model.tokenizer.decode(pred)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "model_names = [\n",
    "    'parseq',\n",
    "    'trba',\n",
    "    'abinet',\n",
    "    'parseq-tiny',\n",
    "    'vitstr',\n",
    "    'crnn']\n",
    "models = [load_from_checkpoint(f\"pretrained={model_name}\").eval() for model_name in model_names]\n",
    "# print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/MH2/PARSeq/test_codes/../demo_images2/Occlusion1.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/View1.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/View2.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Curved1.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Curved2.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Curved3.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Hard1.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Hard2.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Hard3.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Blurring1.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Starbucks (cropped).jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Starbucks  (full).jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Distorted2.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Distorted1.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Distorted3.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Shape1.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/view3.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Dark2.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Dark3.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Shape2.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Doted2.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Doted3.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Shape3.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Occlusion3.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Occlusion2.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Doted1.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Dark1.jpg',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Blurring3.png',\n",
       " '/home/MH2/PARSeq/test_codes/../demo_images2/Blurring2.png']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# samples\n",
    "path_list = glob.glob(str(Path(\"./../demo_images2\").absolute()/\"**\"))\n",
    "path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:07<00:00,  3.77it/s]\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "for path in tqdm(path_list):\n",
    "    name = Path(path).name\n",
    "    result_dict[name] = [do(model, path) for model in models]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blurring1.png [['FOOD'], ['FOOD'], ['FOOD'], ['OOD'], ['FOOD'], ['FOOD']]\n",
      "Blurring2.png [['sma'], ['sma'], ['S.na'], ['sma'], ['sma'], ['5mE']]\n",
      "Blurring3.png [['OPEN'], ['OPEN'], ['OPEN'], ['OPEN'], ['OPEN'], ['OPEN']]\n",
      "Curved1.jpg [['OLDTOWN'], ['OLDTOWN'], ['OLDTOWN'], ['OLDTOWN'], ['OLDTOWN'], ['OLDTOWN']]\n",
      "Curved2.jpg [['COBRA'], ['COBRA'], ['COBRA'], ['COBRA'], ['COBRA'], ['COBRA']]\n",
      "Curved3.jpg [['HISTORIC'], ['HISTORIC'], ['HISTORIC'], ['HISTORIC'], ['HISTORIC'], ['HISTORIO']]\n",
      "Dark1.jpg [['Massa'], ['Massa'], ['Massa'], ['Massa'], ['Massa'], ['Mass']]\n",
      "Dark2.jpg [['SAIGON'], ['SAIGON'], ['SAIGON'], ['SAIGON'], ['SAIGON'], ['SAIGON']]\n",
      "Dark3.jpg [['JIMMY'], ['JIMMY'], ['JIMMY'], ['JIMMY'], ['JIMMY'], ['JIMIMTY']]\n",
      "Distorted1.jpg [['ZOU'], ['ZOU'], ['ZOU'], ['ZOU'], ['ZOU'], ['ZOU']]\n",
      "Distorted2.jpg [['MOTORS'], ['MOTORS'], ['MOTORS'], ['mOTORS'], ['MOTORS'], ['POTORS']]\n",
      "Distorted3.jpg [['DISTORIES'], ['DISTONER'], ['EDISTONEE'], ['DICENSER'], ['DISTATER'], ['PogeEt']]\n",
      "Doted1.png [['BoBa'], ['BoBa'], ['BoBa'], ['BoBa'], ['BoBa'], ['BoBo']]\n",
      "Doted2.png [['24'], ['25'], ['24'], ['24'], ['24'], ['24']]\n",
      "Doted3.png [['GPS'], ['GPS'], ['GPS'], ['GPS'], ['GPS'], ['GPS']]\n",
      "Hard1.png [['Now'], ['Now'], ['Now'], ['Now'], ['Now'], ['NoW']]\n",
      "Hard2.png [['ShiT'], ['Shit'], ['ShiT'], ['Shift'], ['ShiT'], ['Shir']]\n",
      "Hard3.png [['UNIVER'], ['LITTER'], ['LTTER'], ['THE'], ['CTT'], ['THE']]\n",
      "Occlusion1.png [['your'], ['your'], ['your'], ['your'], ['your'], ['your']]\n",
      "Occlusion2.png [['Mobil'], ['Mobil'], ['Mobil'], ['Mobil'], ['Mobil'], ['Mobi']]\n",
      "Occlusion3.png [['ORPHEUM'], ['ORSHEUM'], ['ORPHEUM'], ['ORTHEUM'], ['OREHEUM'], ['ORFHEUM']]\n",
      "Shape1.png [['KEY'], ['KEY'], ['KEY'], ['KEY'], ['KED'], ['KtY']]\n",
      "Shape2.jpg [['Holiday'], ['Holiday'], ['Holiday'], ['Hotiday'], ['Holiday'], ['Hotiday']]\n",
      "Shape3.png [['INDIA'], ['INDIA'], ['INDIA'], ['INDIA'], ['INDIA'], ['INDIA']]\n",
      "Starbucks  (full).jpg [['O'], ['O'], ['O'], ['O'], ['O'], ['C']]\n",
      "Starbucks (cropped).jpg [['STARBUCKS'], ['STARBUCKS'], ['STARBUCKS'], ['STARBUCKS'], ['STARBUCKS'], ['STARBUCKS']]\n",
      "View1.png [['Robert'], ['Robert'], ['Robert'], ['Robert'], ['Robert'], ['Robert']]\n",
      "View2.png [['FOSSIL'], ['FOSSIL'], ['FOSSIL'], ['FOSSIL'], ['FOSSIL'], ['FOSSIL']]\n",
      "view3.jpg [['FEET'], ['FEET'], ['FEET'], ['FEET'], ['FEET'], ['FEET']]\n"
     ]
    }
   ],
   "source": [
    "keys = list(result_dict.keys())\n",
    "keys.sort()\n",
    "for key in keys:\n",
    "\n",
    "    print(key, result_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
